{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dabb90a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdb\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[0;32m     14\u001b[0m matplotlib\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAgg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\matplotlib\\__init__.py:979\u001b[0m\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[0;32m    976\u001b[0m \u001b[38;5;66;03m# When constructing the global instances, we need to perform certain updates\u001b[39;00m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# by explicitly calling the superclass (dict.update, dict.items) to avoid\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# triggering resolution of _auto_backend_sentinel.\u001b[39;00m\n\u001b[1;32m--> 979\u001b[0m rcParamsDefault \u001b[38;5;241m=\u001b[39m \u001b[43m_rc_params_in_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatplotlibrc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Strip leading comment.\u001b[39;49;00m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfail_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(rcParamsDefault, rcsetup\u001b[38;5;241m.\u001b[39m_hardcoded_defaults)\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# Normally, the default matplotlibrc file contains *no* entry for backend (the\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# corresponding line starts with ##, not #; we fill on _auto_backend_sentinel\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;66;03m# in that case.  However, packagers can set a different default backend\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# (resulting in a normal `#backend: foo` line) in which case we should *not*\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# fill in _auto_backend_sentinel.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\matplotlib\\__init__.py:884\u001b[0m, in \u001b[0;36m_rc_params_in_file\u001b[1;34m(fname, transform, fail_on_error)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_or_url(fname) \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 884\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line_no, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fd, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    885\u001b[0m             line \u001b[38;5;241m=\u001b[39m transform(line)\n\u001b[0;32m    886\u001b[0m             strippedline \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39m_strip_comment(line)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##---------------------------------------\n",
    "## Training based on posterior and MMD cost function\n",
    "##_______________________________________\n",
    "\n",
    "import argparse\n",
    "#import _pickle as pickle\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Give the training images from the MNIST dataset\n",
    "\"\"\"\n",
    "def loadMNIST():\n",
    "\n",
    "    # Downloaded from http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "    with open('mnist.pkl', 'rb') as f:    \n",
    "        train_data, val_data, test_data = pickle.load(f,encoding=\"bytes\")\n",
    "    train_x, train_y = train_data\n",
    "\n",
    "    return train_x\n",
    "\"\"\"\n",
    "Give the training images from the histopathology dataset\n",
    "\"\"\"\n",
    "def loadHPTLOG():\n",
    "\n",
    "    # Downloaded from https://github.com/jmtomczak/vae_householder_flow/tree/master/datasets/histopathologyGray\n",
    "    with open('histopathology.pkl', 'rb') as f:    \n",
    "        train_HPTLOG = pickle.load(f,encoding=\"bytes\")\n",
    "    right=train_HPTLOG.values()\n",
    "    right=list(right)        #the sample numbers in realstic samples from generator\n",
    "    right=np.array(right)\n",
    "    HPTLOG=right[1]\n",
    "    HPTLOG=np.array(HPTLOG)\n",
    "    HPTLOG=HPTLOG.reshape([6800,784])\n",
    "\n",
    "    return HPTLOG\n",
    "\n",
    "\"\"\"\n",
    "Give the training images from the MRI_meningioma dataset\n",
    "\"\"\"\n",
    "def loadMRImeningioma():\n",
    "\n",
    "    #Original dataset is given by: https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset\n",
    "    data=np.loadtxt('Training_MRI_meningioma.txt')\n",
    "    data_min=np.min(data, keepdims=True)\n",
    "    data_max=np.max(data, keepdims=True)\n",
    "    scale_data=(data-data_min)/(data_max-data_min)\n",
    "\n",
    "    return scale_data\n",
    "\"\"\"\n",
    "Give the training images from the MRI_meningioma dataset\n",
    "\"\"\"\n",
    "def loadMRI():\n",
    "\n",
    "    #Original dataset is given by: https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset\n",
    "    data=np.loadtxt('Training_MRI_total.txt')\n",
    "    data_min=np.min(data, keepdims=True)\n",
    "    data_max=np.max(data, keepdims=True)\n",
    "    scale_data=(data-data_min)/(data_max-data_min)\n",
    "\n",
    "    return scale_data\n",
    "\"\"\"\n",
    "Give the training images from the cropped LFW dataset\n",
    "\"\"\"\n",
    "def loadLFW():\n",
    "\n",
    "    # 32x32 version of grayscale cropped LFW\n",
    "    # Original dataset is given by: http://conradsanderson.id.au/lfwcrop/\n",
    "    return np.load('lfw.npy')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Posterior of real data (x)\n",
    "\"\"\"\n",
    "def x_pos( x):\n",
    "    from scipy import stats\n",
    "    from numpy.random import choice\n",
    "    # batch size for the training\n",
    "    batch_size = 1000\n",
    "    input_dim=784\n",
    "    # generate images from the provided uniform samples\n",
    "    n=1000 # Determine the number of terms in DP approximation (N)\n",
    "    d=input_dim\n",
    "    m=batch_size\n",
    "    a =.05\n",
    "    g2_pos=np.zeros((1, n)); tstar2_pos =np.zeros((n, d))\n",
    "    g22_pos= np.zeros((1, n)); pp2_pos= np.zeros((1, n))\n",
    "    y21_pos=np.random.exponential(scale=1.0, size=n+1)\n",
    "\n",
    "    for i in range(0, n):\n",
    "        g2_pos[0,i]=np.sum(y21_pos[0:i+1])/np.sum(y21_pos)\n",
    "        g22_pos[0,i]=stats.gamma.ppf(1-g2_pos[0,i], (a+m)/n, 1)-1\n",
    "        u=np.random.uniform(low=0.0, high=1.0, size=1)\n",
    "        if u<a/(a+m):\n",
    "            tstar2_pos[i-1 ,0:d]=np.random.multivariate_normal(np.zeros(d), np.identity(d), 1)\n",
    "        else:\n",
    "            r2=np.random.randint(m, size=1)\n",
    "            tstar2_pos[i-1 ,0:d]=x[r2]\n",
    "\n",
    "\n",
    "    p22_pos=g22_pos/np.sum(g22_pos)\n",
    "    v2_pos=choice(np.arange(0, n, 1, dtype=int), n,p=p22_pos[0])\n",
    "    tstarDP2_pos=tstar2_pos[v2_pos]\n",
    "\n",
    "    X_star2_pos=tstarDP2_pos; J_star2_pos=p22_pos\n",
    "    return X_star2_pos,J_star2_pos\n",
    "\n",
    "batch_size = 1000\n",
    "input_dim    = 784\n",
    "image_side   = 28\n",
    "num_examples = 50000\n",
    "train_x      = loadMNIST()\n",
    "batch_indices = np.random.randint(num_examples, size = batch_size)\n",
    "batch_x       = train_x[batch_indices]\n",
    "posterior     =x_pos( batch_x)\n",
    "batch_pos     =posterior[0]\n",
    "J_star2_pos   =-posterior[1][0]\n",
    "\n",
    "\"\"\"\n",
    "Return a TF variable with zeros of provided shape\n",
    "\"\"\"\n",
    "def zeros(shape):\n",
    "\n",
    "    return tf.Variable(tf.zeros(shape))\n",
    "\n",
    "\"\"\"\n",
    "Return a TF variable with numbers drawn from a normal distribution of zero mean\n",
    "and given standard deviation\n",
    "\"\"\"\n",
    "def normal(shape, std_dev):\n",
    "\n",
    "    return tf.Variable(tf.random.normal(shape, stddev = std_dev))\n",
    "\n",
    "class ReLULayer():\n",
    "\n",
    "    \"\"\"\n",
    "    Initialize layer object with the given input and output dimensions\n",
    "\n",
    "    input_dim:  Dimension of inputs to the layer\n",
    "    output_dim: Dimension of outputs of the layer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "\n",
    "        # initialize weights and biases for the layer\n",
    "        self.W = normal([input_dim, output_dim], 1.0 / math.sqrt(input_dim))\n",
    "        self.b = zeros([output_dim])\n",
    "\n",
    "    \"\"\"\n",
    "    Forward propagation in the layer\n",
    "\n",
    "    x: Input to the layer\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "\n",
    "        return tf.nn.relu(tf.matmul(x, self.W) + self.b)\n",
    "\n",
    "class SigmoidLayer():\n",
    "\n",
    "    \"\"\"\n",
    "    Initialize layer object with the given input, output dimensions and dropout\n",
    "    retention probabilities\n",
    "\n",
    "    input_dim:    Dimension of inputs to the layer\n",
    "    output_dim:   Dimension of outputs of the layer\n",
    "    dropout_prob: Fraction of dropout retention in the layer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, dropout_prob = 1.0):\n",
    "\n",
    "        # initialize weights and biases for the layer\n",
    "        self.W = normal([input_dim, output_dim], 1.0 / math.sqrt(input_dim))\n",
    "        self.b = zeros([output_dim])\n",
    "\n",
    "        # store the dropout retention probability for later use\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    \"\"\"\n",
    "    Forward propagation in the layer\n",
    "\n",
    "    x: Input to the layer\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "\n",
    "        return tf.sigmoid(tf.matmul(tf.nn.dropout(x, rate=1 - (self.dropout_prob)),\n",
    "                          self.W) + self.b)\n",
    "\n",
    "class DataSpaceNetwork():\n",
    "\n",
    "    \"\"\"\n",
    "    Initialize network object with the given dimensions and batch size\n",
    "\n",
    "    dimensions: Dimensions of the all the layers of the network, including\n",
    "                input and output\n",
    "    batch_size: Number of training examples taken in the batch\n",
    "    \"\"\"\n",
    "    def __init__(self, dimensions, batch_size):\n",
    "\n",
    "        # store 'dimensions' and 'batch_size' for later use\n",
    "        self.dimensions = dimensions\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # store the layers as a list\n",
    "        self.layers = []\n",
    "\n",
    "        # all the layers except the last one is 'ReLU'\n",
    "        for dim_index in range(len(dimensions)-2):\n",
    "            self.layers.append(ReLULayer(dimensions[dim_index],\n",
    "                                         dimensions[dim_index+1]))\n",
    "\n",
    "        # last layer is 'Sigmoid' as we need the outputs to be in [0, 1]\n",
    "        self.layers.append(SigmoidLayer(dimensions[dim_index+1],\n",
    "                                        dimensions[dim_index+2]))\n",
    "\n",
    "    \"\"\"\n",
    "    Forward propagation of the network\n",
    "\n",
    "    x: Input batch of samples from the uniform\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "\n",
    "        # initialize the first 'hidden' layer to the input\n",
    "        h = x\n",
    "\n",
    "        # for all the layers propagate the activation forward\n",
    "        # all layers have the 'forward()' method\n",
    "        for dim_index in range(len(self.dimensions)-1):\n",
    "            h = self.layers[dim_index].forward(h)\n",
    "\n",
    "        return h\n",
    "    \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Scale column for the MMD measure\n",
    "\n",
    "    num_gen:  Number of samples to be generated in one pass, 'N' in the paper\n",
    "    num_orig: Number of samples taken from dataset in one pass, 'M' in the paper\n",
    "    \"\"\"\n",
    "    def makeScaleMatrix(self, num_gen, num_orig):\n",
    "\n",
    "        # first 'N' entries have '1/N', next 'M' entries have '-1/M'\n",
    "        s1 =  tf.constant(1.0 / num_gen, shape = [num_gen, 1])\n",
    "        s2 = -tf.constant(1.0 / num_orig, shape = [num_orig, 1])\n",
    "\n",
    "        return tf.concat([s1, s2], axis=0)\n",
    "    \n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    Calculates cost of the network, which is square root of the mixture of 'K'\n",
    "    RBF kernels\n",
    "\n",
    "    x:       Batch from the dataset\n",
    "    samples: Samples from the uniform distribution\n",
    "    sigma:   Bandwidth parameters for the 'K' kernels\n",
    "    \"\"\"\n",
    "    def computeLoss(self, x, samples, weight, sigma = [2,5,10,20,40,80]):\n",
    "\n",
    "\n",
    "        # generate images from the provided uniform samples\n",
    "        gen_x = self.forward(samples)\n",
    "\n",
    "\n",
    "        # concatenation of the generated images and images from the dataset\n",
    "        # first 'N' rows are the generated ones, next 'M' are from the data\n",
    "        X = tf.concat([gen_x, x], axis=0)\n",
    "\n",
    "        # dot product between all combinations of rows in 'X'\n",
    "        XX = tf.matmul(X, tf.transpose(a=X))\n",
    "\n",
    "        # dot product of rows with themselves\n",
    "        X2 = tf.reduce_sum(input_tensor=X * X, axis=1, keepdims = True)\n",
    "\n",
    "        # exponent entries of the RBF kernel (without the sigma) for each\n",
    "        # combination of the rows in 'X'\n",
    "        # -0.5 * (x^Tx - 2*x^Ty + y^Ty)\n",
    "        exponent = XX -.5*  X2 -.5* tf.transpose(a=X2)\n",
    "\n",
    "        # scaling constants for each of the rows in 'X'\n",
    "        \n",
    "        #J1=tf.reshape(J_star1_pos,[batch_size,1])\n",
    "        J2=tf.reshape(weight,[batch_size,1])\n",
    "        s1 =  tf.constant(1.0 / self.batch_size, shape = [self.batch_size, 1])\n",
    "        s =tf.concat([s1,tf.cast(J2, tf.float32)], axis=0) #self.makeScaleMatrix(self.batch_size, self.batch_size)\n",
    "\n",
    "        # scaling factors of each of the kernel values, corresponding to the\n",
    "        # exponent values\n",
    "        S = tf.matmul(s, tf.transpose(a=s))\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        # for each bandwidth parameter, compute the MMD value and add them all\n",
    "        for i in range(len(sigma)):\n",
    "\n",
    "            # kernel values for each combination of the rows in 'X' \n",
    "            kernel_val = tf.exp(1.0 / sigma[i] * exponent)\n",
    "            loss += tf.reduce_sum(input_tensor=S * kernel_val)\n",
    "\n",
    "        return tf.sqrt(loss)\n",
    "def generateFigure(samples, num_rows, num_cols, image_side, file_name):\n",
    "\n",
    "    # initialize the figure object\n",
    "    figure, axes = plt.subplots(nrows = num_rows, ncols = num_cols)\n",
    "\n",
    "    index = 0\n",
    "    # take the first 'num_rows * num_cols' samples from the provided batch\n",
    "    for axis in axes.flat:\n",
    "        image = axis.imshow(samples[index, :].reshape(image_side, image_side),\n",
    "                            cmap = plt.cm.gray, interpolation = 'nearest')\n",
    "        axis.set_frame_on(False)\n",
    "        axis.set_axis_off()\n",
    "        index += 1 \n",
    "\n",
    "    # save the figure\n",
    "    figure.savefig(file_name)\n",
    "def trainDataSpaceNetwork(dataset):\n",
    "\n",
    "    # batch size for the training\n",
    "    batch_size = 1000\n",
    "    \n",
    "    # parameters and training set for MNIST\n",
    "    if dataset == 'mnist':\n",
    "        input_dim    = 784\n",
    "        image_side   = 28\n",
    "        num_examples = 50000\n",
    "        train_x      = loadMNIST()\n",
    "\n",
    "    # parameters and training set for LFW\n",
    "    elif dataset == 'lfw':\n",
    "        input_dim    = 1024\n",
    "        image_side   = 32\n",
    "        num_examples = 13000\n",
    "        train_x      = loadLFW()\n",
    "    # parameters and training set for histopathology\n",
    "    elif dataset == 'histopathology':\n",
    "        input_dim    = 784\n",
    "        image_side   = 28\n",
    "        num_examples = 6800\n",
    "        train_x      = loadHPTLOG()\n",
    "    elif dataset == 'MRImeningioma':\n",
    "        input_dim    = 2500\n",
    "        image_side   = 50\n",
    "        num_examples = 1339\n",
    "        train_x      = loadMRImeningioma()\n",
    "    elif dataset == 'MRI':\n",
    "        input_dim    = 2500\n",
    "        image_side   = 50\n",
    "        num_examples = 5712\n",
    "        train_x      = loadMRI()\n",
    "\n",
    "    # dimensions of the moment matching network\n",
    "    data_space_dims = [10, 64, 256, 256, input_dim]\n",
    "\n",
    "    # get a DataSpaceNetwork object\n",
    "    data_space_network = DataSpaceNetwork(data_space_dims, batch_size)\n",
    "\n",
    "    # placeholders for the data batch and the uniform samples respectively\n",
    "    x       = tf.compat.v1.placeholder(\"float\", [batch_size, input_dim])\n",
    "    samples = tf.compat.v1.placeholder(\"float\", [batch_size, data_space_dims[0]])\n",
    "    weight  = tf.compat.v1.placeholder(\"float\", [batch_size, 1])\n",
    "    \n",
    "    # cost of the network, and optimizer for the cost\n",
    "    cost      = data_space_network.computeLoss(x, samples, weight)\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    # generator for the network\n",
    "    generate = data_space_network.forward(samples)\n",
    "\n",
    "    # initalize all the variables in the model\n",
    "    init = tf.compat.v1.initialize_all_variables()\n",
    "    sess = tf.compat.v1.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    # number of batches to train the model on, and frequency of printing out the\n",
    "    # cost\n",
    "    num_iterations  = 40001\n",
    "    iteration_break = 1000\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # sample a random batch from the training set\n",
    "        batch_indices = np.random.randint(num_examples, size = batch_size)\n",
    "        batch_x       = train_x[batch_indices]\n",
    "        posterior     = x_pos( batch_x)\n",
    "        batch_pos     = posterior[0]\n",
    "        J_star2_pos   = -posterior[1][0].reshape(-1,1)\n",
    "        #batch_x       = train_x[batch_indices]\n",
    "        batch_uniform = np.random.uniform(low = -1.0, high = 1.0,\n",
    "            size = (batch_size, data_space_dims[0]))\n",
    "\n",
    "        \n",
    "        # print out the cost after every 'iteration_break' iterations\n",
    "        if i % iteration_break == 0:\n",
    "            curr_cost = sess.run(cost, feed_dict = {samples: batch_uniform,\n",
    "                                                    x: batch_pos, weight: J_star2_pos})\n",
    "            print('Cost at iteration ' + str(i+1) + ': ' + str(curr_cost))\n",
    "\n",
    "        # optimize the network\n",
    "        sess.run(optimizer, feed_dict = {samples: batch_uniform, x: batch_pos, \n",
    "                                         weight: J_star2_pos})\n",
    "\n",
    "    # parameters for figure generation\n",
    "    num_rows = 10; num_cols = 10\n",
    "\n",
    "    # generate samples from the trained network\n",
    "    batch_uniform = np.random.uniform(low = -1.0, high = 1.0,\n",
    "        size = (batch_size, data_space_dims[0]))\n",
    "    gen_samples   = sess.run(generate, feed_dict = {samples: batch_uniform})\n",
    "\n",
    "    # generate figure of generated samples\n",
    "    file_name = dataset + '_data_space_MMD_pos40000.png'\n",
    "    figg=generateFigure(gen_samples, num_rows, num_cols, image_side, file_name)\n",
    "    return gen_samples,figg\n",
    "\n",
    "\"\"\"\n",
    "Train code space network on the given dataset\n",
    "\n",
    "dataset: Either 'mnist' or 'lfw', indicating the dataset\n",
    "\"\"\"    \n",
    "out=trainDataSpaceNetwork('mnist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a5337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
